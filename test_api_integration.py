#!/usr/bin/env python3
"""
Integration test for all tools with actual API calls.
Tests end-to-end functionality to ensure Phase 2 is complete.
"""

import sys
import os
import asyncio
from typing import Dict, Any
import traceback

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

async def test_writing_tools_integration():
    """Test writing tools with actual OpenAI API calls."""
    print("\n‚úçÔ∏è TESTING WRITING TOOLS INTEGRATION")
    print("="*50)
    
    results = {}
    
    # Test Content Writer
    print("\nüìù Testing Content Writer with OpenAI:")
    try:
        import openai
        
        client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # Test content generation
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{
                "role": "user", 
                "content": "Write a brief 100-word article about artificial intelligence in content creation."
            }],
            max_tokens=150,
            temperature=0.7
        )
        
        generated_content = response.choices[0].message.content
        
        print(f"‚úÖ Content Writer working perfectly")
        print(f"   - Generated {len(generated_content)} characters")
        print(f"   - Preview: {generated_content[:100]}...")
        
        results['content_writer'] = {
            'status': 'success',
            'content_length': len(generated_content),
            'model_used': 'gpt-3.5-turbo'
        }
        
    except Exception as e:
        print(f"‚ùå Content Writer failed: {e}")
        results['content_writer'] = {'status': 'error', 'error': str(e)}
    
    # Test Headline Generator
    print("\nüéØ Testing Headline Generator:")
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{
                "role": "user",
                "content": "Generate 3 compelling headlines for an article about AI in content creation. Make them engaging and click-worthy."
            }],
            max_tokens=100,
            temperature=0.8
        )
        
        headlines = response.choices[0].message.content
        headline_list = [h.strip() for h in headlines.split('\n') if h.strip()]
        
        print(f"‚úÖ Headline Generator working")
        print(f"   - Generated {len(headline_list)} headlines")
        for i, headline in enumerate(headline_list[:3], 1):
            print(f"   - {i}: {headline}")
        
        results['headline_generator'] = {
            'status': 'success',
            'headlines_generated': len(headline_list)
        }
        
    except Exception as e:
        print(f"‚ùå Headline Generator failed: {e}")
        results['headline_generator'] = {'status': 'error', 'error': str(e)}
    
    # Test Image Generator (DALL-E)
    print("\nüé® Testing Image Generator:")
    try:
        # Test DALL-E image generation
        image_response = client.images.generate(
            model="dall-e-3",
            prompt="A modern, minimalist illustration of artificial intelligence helping with content creation, featuring digital elements and writing symbols",
            size="1024x1024",
            quality="standard",
            n=1
        )
        
        image_url = image_response.data[0].url
        
        print(f"‚úÖ Image Generator working")
        print(f"   - Generated image URL: {image_url[:50]}...")
        print(f"   - Model: DALL-E 3")
        print(f"   - Size: 1024x1024")
        
        results['image_generator'] = {
            'status': 'success',
            'image_url': image_url,
            'model': 'dall-e-3'
        }
        
    except Exception as e:
        print(f"‚ùå Image Generator failed: {e}")
        results['image_generator'] = {'status': 'error', 'error': str(e)}
    
    return results

async def test_research_tools_integration():
    """Test research tools with actual API calls."""
    print("\nüìä TESTING RESEARCH TOOLS INTEGRATION")
    print("="*50)
    
    results = {}
    
    # Test Web Search with SerpAPI
    print("\nüîç Testing Web Search with SerpAPI:")
    try:
        from serpapi import GoogleSearch
        
        search = GoogleSearch({
            "q": "artificial intelligence content writing 2024",
            "api_key": os.getenv('SERPAPI_KEY'),
            "num": 5
        })
        
        search_results = search.get_dict()
        organic_results = search_results.get('organic_results', [])
        
        print(f"‚úÖ Web Search working")
        print(f"   - Found {len(organic_results)} results")
        for i, result in enumerate(organic_results[:3], 1):
            title = result.get('title', 'No title')[:50]
            print(f"   - {i}: {title}...")
        
        results['web_search'] = {
            'status': 'success',
            'results_found': len(organic_results),
            'api': 'SerpAPI'
        }
        
    except Exception as e:
        print(f"‚ùå Web Search failed: {e}")
        results['web_search'] = {'status': 'error', 'error': str(e)}
    
    # Test News Search
    print("\nüì∞ Testing News Search:")
    try:
        from newsapi import NewsApiClient
        
        newsapi = NewsApiClient(api_key=os.getenv('NEWS_API_KEY'))
        
        # Try top headlines first, fallback to everything search
        try:
            top_headlines = newsapi.get_top_headlines(
                q='artificial intelligence',
                language='en',
                page_size=5
            )
            articles = top_headlines.get('articles', [])
            search_type = "top headlines"
        except:
            articles = []
        
        # If no top headlines, try everything search
        if len(articles) == 0:
            everything = newsapi.get_everything(
                q='AI OR "artificial intelligence"',
                language='en',
                sort_by='publishedAt',
                page_size=5
            )
            articles = everything.get('articles', [])
            search_type = "everything search"
        else:
            search_type = "top headlines"
        
        print(f"‚úÖ News Search working")
        print(f"   - Found {len(articles)} news articles via {search_type}")
        for i, article in enumerate(articles[:3], 1):
            title = article.get('title', 'No title')[:50]
            print(f"   - {i}: {title}...")
        
        results['news_search'] = {
            'status': 'success',
            'articles_found': len(articles),
            'search_type': search_type,
            'api': 'NewsAPI'
        }
        
    except Exception as e:
        print(f"‚ùå News Search failed: {e}")
        results['news_search'] = {'status': 'error', 'error': str(e)}
    
    # Test Trend Finder
    print("\nüìà Testing Trend Finder:")
    try:
        from pytrends.request import TrendReq
        
        pytrends = TrendReq(hl='en-US', tz=360)
        
        # Build payload for keywords
        keywords = ['artificial intelligence', 'content writing']
        pytrends.build_payload(keywords, cat=0, timeframe='today 1-m', geo='', gprop='')
        
        # Get interest over time
        interest_data = pytrends.interest_over_time()
        
        if not interest_data.empty:
            print(f"‚úÖ Trend Finder working")
            print(f"   - Data points: {len(interest_data)}")
            print(f"   - Keywords analyzed: {keywords}")
            print(f"   - Latest trends available")
            
            results['trend_finder'] = {
                'status': 'success',
                'data_points': len(interest_data),
                'keywords': keywords
            }
        else:
            print(f"‚ö†Ô∏è Trend Finder: No data returned")
            results['trend_finder'] = {
                'status': 'no_data',
                'keywords': keywords
            }
        
    except Exception as e:
        print(f"‚ùå Trend Finder failed: {e}")
        results['trend_finder'] = {'status': 'error', 'error': str(e)}
    
    # Test Content Retrieval
    print("\nüìÑ Testing Content Retrieval:")
    try:
        import requests
        from bs4 import BeautifulSoup
        
        # Test retrieving content from a real website
        url = "https://httpbin.org/html"  # Test HTML endpoint
        response = requests.get(url, timeout=10)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            title = soup.find('title')
            content_length = len(soup.get_text().strip())
            
            print(f"‚úÖ Content Retrieval working")
            print(f"   - URL: {url}")
            print(f"   - Status: {response.status_code}")
            print(f"   - Content length: {content_length} chars")
            print(f"   - Title extracted: {'Yes' if title else 'No'}")
            
            results['content_retrieval'] = {
                'status': 'success',
                'url_tested': url,
                'content_length': content_length,
                'has_title': bool(title)
            }
        else:
            print(f"‚ö†Ô∏è Content Retrieval: HTTP {response.status_code}")
            results['content_retrieval'] = {
                'status': 'http_error',
                'status_code': response.status_code
            }
        
    except Exception as e:
        print(f"‚ùå Content Retrieval failed: {e}")
        results['content_retrieval'] = {'status': 'error', 'error': str(e)}
    
    return results

async def test_reddit_integration():
    """Test Reddit Search integration."""
    print("\nüó£Ô∏è TESTING REDDIT INTEGRATION")
    print("="*50)
    
    try:
        import praw
        
        reddit = praw.Reddit(
            client_id=os.getenv('REDDIT_CLIENT_ID'),
            client_secret=os.getenv('REDDIT_CLIENT_SECRET'),
            user_agent=os.getenv('REDDIT_USER_AGENT')
        )
        
        # Test by getting a few posts from a popular subreddit
        subreddit = reddit.subreddit('artificial')
        posts = list(subreddit.hot(limit=3))
        
        print(f"‚úÖ Reddit API working")
        print(f"   - Accessed subreddit: r/artificial")
        print(f"   - Retrieved {len(posts)} posts")
        for i, post in enumerate(posts, 1):
            title = post.title[:50] if hasattr(post, 'title') else 'No title'
            print(f"   - {i}: {title}...")
        
        return {
            'status': 'success',
            'subreddit_tested': 'artificial',
            'posts_retrieved': len(posts)
        }
        
    except Exception as e:
        print(f"‚ùå Reddit integration failed: {e}")
        return {'status': 'error', 'error': str(e)}

async def run_complete_workflow_test():
    """Test a complete content creation workflow using multiple tools."""
    print("\nüîÑ TESTING COMPLETE WORKFLOW")
    print("="*50)
    
    workflow_results = {}
    
    try:
        # Step 1: Research phase
        print("\nüìä Step 1: Research Phase")
        
        # Use trend finder for topic research
        from pytrends.request import TrendReq
        pytrends = TrendReq(hl='en-US', tz=360)
        
        topic = "AI content creation"
        pytrends.build_payload([topic], timeframe='today 1-m')
        interest_data = pytrends.interest_over_time()
        
        print(f"   ‚úÖ Topic research: {topic}")
        print(f"   - Trend data points: {len(interest_data) if not interest_data.empty else 0}")
        
        # Step 2: Content creation
        print("\n‚úçÔ∏è Step 2: Content Creation")
        
        import openai
        client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # Generate content
        content_response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{
                "role": "user",
                "content": f"Write a brief 150-word article about {topic} for a business audience."
            }],
            max_tokens=200,
            temperature=0.7
        )
        
        generated_content = content_response.choices[0].message.content
        
        print(f"   ‚úÖ Content generated: {len(generated_content)} characters")
        
        # Step 3: Analysis and optimization
        print("\nüìà Step 3: Analysis & Optimization")
        
        # Sentiment analysis
        from textblob import TextBlob
        blob = TextBlob(generated_content)
        sentiment = blob.sentiment.polarity
        
        # Readability analysis
        import textstat
        readability = textstat.flesch_reading_ease(generated_content)
        
        print(f"   ‚úÖ Content analysis complete")
        print(f"   - Sentiment: {sentiment:.2f}")
        print(f"   - Readability: {readability:.1f}")
        
        # Step 4: SEO optimization
        print("\nüîç Step 4: SEO Optimization")
        
        from bs4 import BeautifulSoup
        
        # Simulate HTML content
        html_content = f"""
        <html>
        <head><title>{topic.title()} Guide</title></head>
        <body>
        <h1>The Future of {topic.title()}</h1>
        <p>{generated_content}</p>
        </body>
        </html>
        """
        
        soup = BeautifulSoup(html_content, 'html.parser')
        title = soup.find('title').text
        headings = len(soup.find_all(['h1', 'h2', 'h3']))
        
        print(f"   ‚úÖ SEO analysis complete")
        print(f"   - Title: {title}")
        print(f"   - Headings: {headings}")
        
        workflow_results = {
            'status': 'success',
            'research': {'topic': topic, 'trend_data': len(interest_data) if not interest_data.empty else 0},
            'content': {'length': len(generated_content), 'model': 'gpt-3.5-turbo'},
            'analysis': {'sentiment': sentiment, 'readability': readability},
            'seo': {'title': title, 'headings': headings}
        }
        
        print(f"\nüéâ Complete workflow test: SUCCESS")
        
    except Exception as e:
        print(f"‚ùå Workflow test failed: {e}")
        workflow_results = {'status': 'error', 'error': str(e)}
    
    return workflow_results

async def main():
    """Run comprehensive integration tests."""
    print("üöÄ CONTENT WRITING AGENTS - API INTEGRATION TESTING")
    print("Testing all tools with live API calls")
    print("="*60)
    
    try:
        # Test writing tools
        writing_results = await test_writing_tools_integration()
        
        # Test research tools  
        research_results = await test_research_tools_integration()
        
        # Test Reddit
        reddit_results = await test_reddit_integration()
        
        # Test complete workflow
        workflow_results = await run_complete_workflow_test()
        
        # Combine all results
        all_results = {
            'writing_tools': writing_results,
            'research_tools': research_results,
            'reddit_integration': reddit_results,
            'complete_workflow': workflow_results
        }
        
        # Generate comprehensive summary
        print("\n" + "="*60)
        print("üìã API INTEGRATION TEST SUMMARY")
        print("="*60)
        
        # Count successes
        total_tests = 0
        successful_tests = 0
        
        for category, tests in all_results.items():
            if category == 'complete_workflow':
                total_tests += 1
                if isinstance(tests, dict) and tests.get('status') == 'success':
                    successful_tests += 1
                continue
                
            for test_name, result in tests.items():
                total_tests += 1
                # Handle case where result might be a string or other type
                if isinstance(result, dict) and result.get('status') == 'success':
                    successful_tests += 1
                elif isinstance(result, str) and result == 'success':
                    successful_tests += 1
        
        success_rate = (successful_tests / total_tests) * 100 if total_tests > 0 else 0
        
        print(f"\nüéØ INTEGRATION RESULTS:")
        print(f"   ‚Ä¢ Total API tests: {total_tests}")
        print(f"   ‚Ä¢ Successful tests: {successful_tests} ({success_rate:.1f}%)")
        
        # Detailed results
        print(f"\nüìä DETAILED RESULTS:")
        
        print(f"\n‚úçÔ∏è Writing Tools:")
        for tool, result in writing_results.items():
            if isinstance(result, dict):
                status_icon = "‚úÖ" if result.get('status') == 'success' else "‚ùå"
                status = result.get('status', 'unknown')
            else:
                status_icon = "‚úÖ" if result == 'success' else "‚ùå"
                status = result
            print(f"   {status_icon} {tool}: {status}")
        
        print(f"\nüìä Research Tools:")
        for tool, result in research_results.items():
            if isinstance(result, dict):
                status_icon = "‚úÖ" if result.get('status') == 'success' else "‚ùå"
                status = result.get('status', 'unknown')
            else:
                status_icon = "‚úÖ" if result == 'success' else "‚ùå"
                status = result
            print(f"   {status_icon} {tool}: {status}")
        
        reddit_icon = "‚úÖ" if (isinstance(reddit_results, dict) and reddit_results.get('status') == 'success') else "‚ùå"
        print(f"\nüó£Ô∏è Social Media:")
        reddit_status = reddit_results.get('status', 'unknown') if isinstance(reddit_results, dict) else reddit_results
        print(f"   {reddit_icon} reddit_search: {reddit_status}")
        
        workflow_icon = "‚úÖ" if (isinstance(workflow_results, dict) and workflow_results.get('status') == 'success') else "‚ùå"
        print(f"\nüîÑ Complete Workflow:")
        workflow_status = workflow_results.get('status', 'unknown') if isinstance(workflow_results, dict) else workflow_results
        print(f"   {workflow_icon} end_to_end_test: {workflow_status}")
        
        # Final assessment
        print(f"\nüèÜ FINAL ASSESSMENT:")
        if success_rate >= 90:
            print("   üéâ EXCELLENT! All systems operational")
            print("   ‚úÖ Ready for Phase 3: Agent Development")
            print("   üöÄ All tools tested and verified with live APIs")
        elif success_rate >= 75:
            print("   üëç GOOD! Most systems working well")
            print("   ‚ö†Ô∏è Minor issues detected, but ready to proceed")
        else:
            print("   üîß NEEDS ATTENTION: Several API issues detected")
            print("   ‚ùó Recommend fixing issues before Phase 3")
        
        return all_results
        
    except Exception as e:
        print(f"‚ùå Integration test failed: {e}")
        traceback.print_exc()
        return None

if __name__ == "__main__":
    results = asyncio.run(main())